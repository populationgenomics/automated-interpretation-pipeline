# Comparison

Comparison of the results generated by this process against an external 'truth' will be crucial to successful
development and refinement. This will help us understand results in terms of Sensitivity/Specificity, and crucially
to understand _why_ expected variants are missing from the final outputs. Where we can pinpoint why variants evade the
current category/MOI assignment, this can lead us to either de-bugging faulty implementation, or to refine the design of
parts of the algorithm.

There are 3 key phases of this analysis where variants can 'drop-out', and identification of these will help undestand,
and potentially resolve any discrepancies against the Truth dataset:

1. During Variant calling: it's possible that true variants were never present in the joint call
    - resolution here would be through returning to the calling pipeline, or raw data
2. During category assignment: variants can drop out due to QC issues, or lack of relevant annotations
   - expected variants may not be present on the panel of interest
   - variants could fail one of the quality filters (AB ratio, Variant Caller assigned `Filters`, too common within the
   joint-callset)
   - annotations may not pass one of the defined categories (e.g. no VEP `HIGH` consequence anno., AF too high in
   Gnomad/ExAC, ClinVar results are conflicting)
3. During MOI checking: variants may be categorised, but may not fit within the PanelApp-sourced MOI
   - e.g. a strong Het. will be excluded if the PanelApp MOI is Biallelic only
   - variant may not segregate appropriately within the family

## Inputs

1. Input representing the AIP results will be provided using the summary JSON format
2. 'Truth' data will be provided using the Seqr tagged variant export table, but could be generalised

Input data from both sources will be translated into a common format:

```python
results = {
   'sample_ID': [
        Variant('chr', 'pos', 'ref', 'alt', 'confidence'),
        Variant('chr', 'pos', 'ref', 'alt', 'confidence'),
    ],
   'sample_ID_2':[
        Variant('chr', 'pos', 'ref', 'alt', 'confidence'),
        ...
   ]
}
```

In addition, we require a number of other input files from the AIP run being analysed:

- PanelApp Data
- VCF produced by the labelling process
- MT used to derive labelled variants from
- Configuration used

---

## Process

A brief algo of the comparison process - this is a tree-style walking through variants, first at a high level `is this
variant matched successfully, or not`, `is this variant in the labelled VCF, or not`, then for variants not yet
explained the searches become progressively more specific: `which element of the quality filter failed`, `which element
of the Category 1 filter criteria was failed`.

1. Parse both AIP and external results into a common format
2. Compare AIP + Ext. data, find `True Positives`
3. For each Discrepancy:

![ComparisonTree](images/comparison_decision_tree.png)

At a more granular level, a check will run for each category, initially testing whether a category flag was applied, then testing each separate component of that category logic if not:

![CategoryTree](images/category_testing_decision_tree.png)

For each failing test, a descriptive String will be generated. The collection of Strings should explain the reason for each variant avoiding categorisation, e.g.

```python
reasons = [
   'Cat. 1: Clinvar rating: Benign',
   'Cat. 1: Clinvar Stars: 1',
   'Cat. 2: Gene not new in PanelApp',
   'Cat. 3: No VEP HIGH CSQ',
   'Cat. 4: Not implemented',
   'Cat. Support: not Missense'
]
```

The summary of all reasons will be exported, and when the comparison detects that a cateogry should have been assigned, this will be flagged for manual review.
